{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EDA_for_NLP_package.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcKrkPLONp-f"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "97FxMj61OVrc",
        "outputId": "32e48a94-fe98-439e-cbb8-926fd78b77cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNeeded instalations\\n! pip install swifter &> /dev/null\\n! pip install spacy &> /dev/null\\n! pip install textacy &> /dev/null\\n! python -m spacy download en_core_web_md &> /dev/null\\n! pip install spacytextblob &> /dev/null\\n! pip install pdfminer &> /dev/null\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "\"\"\"\n",
        "Needed instalations\n",
        "! pip install swifter &> /dev/null\n",
        "! pip install spacy &> /dev/null\n",
        "! pip install textacy &> /dev/null\n",
        "! python -m spacy download en_core_web_md &> /dev/null\n",
        "! pip install spacytextblob &> /dev/null\n",
        "! pip install pdfminer &> /dev/null\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yCdUby6WNcnM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e564169f-1c96-4d48-df4e-5bf4a367dcf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm \n",
        "import swifter\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import gensim\n",
        "import string\n",
        "import textacy\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from matplotlib import pyplot\n",
        "import seaborn as sns\n",
        "pd.options.plotting.backend = \"plotly\"\n",
        "import csv\n",
        "import math\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib.pyplot import figure\n",
        "import pdfminer\n",
        "from spacytextblob.spacytextblob import SpacyTextBlob\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from google.colab import drive\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en = spacy.load(\"en_core_web_md\")\n",
        "en.add_pipe('spacytextblob')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0mp2YGcVyCM",
        "outputId": "0588805d-3abe-4279-b171-cad29b475b63"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacytextblob.spacytextblob.SpacyTextBlob at 0x7f06ea7ea190>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4xlvXxVOgsL"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "o0W-35dGkQx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_pdf_to_txt(path):\n",
        "  \"\"\"\n",
        "  Desc:   converts pdf to the string\n",
        "  Input:  path to the pdf file\n",
        "  Output: string contatining text of the given file\n",
        "  \"\"\"\n",
        "  rsrcmgr = pdfminer.PDFResourceManager()\n",
        "  retstr = pdfminer.StringIO()\n",
        "  codec = 'utf-8'\n",
        "  laparams = pdfminer.LAParams()\n",
        "  device = pdfminer.TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
        "  fp = open(path, 'rb')\n",
        "  interpreter = pdfminer.PDFPageInterpreter(rsrcmgr, device)\n",
        "  password = \"\"\n",
        "  maxpages = 0\n",
        "  caching = True\n",
        "  pagenos=set()\n",
        "\n",
        "  for page in pdfminer.PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
        "      interpreter.process_page(page)\n",
        "\n",
        "  text = retstr.getvalue()\n",
        "\n",
        "  fp.close()\n",
        "  device.close()\n",
        "  retstr.close()\n",
        "  return text"
      ],
      "metadata": {
        "id": "MtEqKdBRSUBa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_pdf_folder_to_text(path,initial_path,w_path):\n",
        "  \"\"\"\n",
        "  Desc:   Converts all pdf files in the given path to text and saves them as\n",
        "          txt files in the w_path folder\n",
        "  Input:  path to the folder contatining pdfs\n",
        "          initial_path to the folder of project\n",
        "          w_path to the folder wehere txt files are saved\n",
        "  Output: saved txt files\n",
        "  \"\"\"\n",
        "  docs2 = []\n",
        "  names = []\n",
        "  #path = \"/content/drive/MyDrive/Commission Adoption Feedback/Pdf\"\n",
        "  #initial_path = \"/content\"\n",
        "  os.chdir(path)\n",
        "\n",
        "  for file in os.listdir():\n",
        "      # Check whether file is in text format or not\n",
        "      if file.endswith(\".pdf\"):\n",
        "          file_path = f\"{path}/{file}\"\n",
        "          names.append(file)\n",
        "          #print(file_path)\n",
        "          # call read text file function\n",
        "          os.chdir(initial_path)\n",
        "          text = convert_pdf_to_txt(file_path)\n",
        "          #print(text_file)\n",
        "          docs2.append(text)\n",
        "  os.chdir(initial_path)\n",
        "  #w_path = \"/content/drive/MyDrive/Commission Adoption Feedback/Text/\"\n",
        "  for i in range(len(docs2)):\n",
        "    with open( w_path + names[i].replace(\".pdf\",\"\") + '.txt', 'w') as f:\n",
        "      f.write(docs2[i])"
      ],
      "metadata": {
        "id": "TqXJWU_TS753"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_txt(path,initial_path):\n",
        "  \"\"\"\n",
        "  Desc:   Reads all txt files in the folder given by path\n",
        "  Input:  path to the folder contatining txt files\n",
        "          initial_path to the folder of project\n",
        "  Output: docs list with texts as strings\n",
        "          names list containing names of the files\n",
        "  \"\"\"\n",
        "  docs = []\n",
        "  names = []\n",
        "  #path = \"/content/drive/MyDrive/Commission Adoption Feedback/Text\"\n",
        "  #initial_path = \"/content\"\n",
        "  os.chdir(path)\n",
        "\n",
        "  for file in os.listdir():\n",
        "      # Check whether file is in text format or not\n",
        "      if file.endswith(\".txt\"):\n",
        "          file_path = f\"{path}/{file}\"\n",
        "          names.append(file)\n",
        "          #print(file_path)\n",
        "          # call read text file function\n",
        "          os.chdir(initial_path)\n",
        "          text_file = open(file_path, \"r\")\n",
        "          #print(text_file)\n",
        "          data = text_file.read()\n",
        "          docs.append(data)\n",
        "  os.chdir(initial_path)\n",
        "  return docs,names"
      ],
      "metadata": {
        "id": "1TptQK2XTmoz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_df(docs,names,min_word_count=250):\n",
        "  \"\"\"\n",
        "  Desc:   Prepares dataframe from docs and names lists with additional statistics.\n",
        "          Dataframe consists of 5 columns: Name, Text, word_count, char_count and \n",
        "          word_density.\n",
        "  Input:  docs list with texts as strings\n",
        "          names list containing names of the files\n",
        "          min_word_count files with smaller word_count are removed from dataframe\n",
        "  Output: df DataFrame object with 5 columns: Name, Text, word_count, char_count and \n",
        "          word_density.\n",
        "  \"\"\"\n",
        "  docs2 = []\n",
        "  for doc in docs:\n",
        "      docs2.append(doc.replace(\"\\n\" or \"\\u200b\", \" \"))\n",
        "\n",
        "  d = {\"Name\": names, \"Text\": docs2}\n",
        "  df = pd.DataFrame(d)\n",
        "  df['word_count'] = df[\"Text\"].apply(lambda x : len(x.split()))\n",
        "  df['char_count'] = df['Text'].apply(lambda x : len(x.replace(\" \",\"\")))\n",
        "  df['word_density'] = df['word_count'] / (df['char_count'] + 1)\n",
        "  df = df[df[\"word_count\"]>min_word_count]\n",
        "  return df"
      ],
      "metadata": {
        "id": "Pb2ARR0PUSAr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pmt6ZTQkTanc"
      },
      "outputs": [],
      "source": [
        "def my_lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_URL(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'',text)\n",
        "\n",
        "def remove_html(text):\n",
        "    html=re.compile(r'<.*?>')\n",
        "    return html.sub(r'',text)\n",
        "\n",
        "def remove_punct(text):\n",
        "    table=str.maketrans('','',string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "def custom_regex(text):\n",
        "    ref=re.compile(r'ref|\\.')\n",
        "    ares=re.compile(r'ares|\\(20(?:00|1[09]|2[01])\\)')\n",
        "    com=re.compile(r'com\\([0-9]{1,4}\\)')\n",
        "    bignum = re.compile(r'[0-9]{5,30}')\n",
        "    parenth = re.compile(r'\\([0-9]{1,4}\\)')\n",
        "    email = re.compile(r'(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)')\n",
        "    listing = re.compile(r'\\([1-3]\\)|\\[[1-3]\\]|[1-3]\\.')\n",
        "    page1 = re.compile(r'Page [0-9]{1,2}(?: \\/ |\\/)[0-9]{1,2}}')\n",
        "    page2 = re.compile(r'Page [0-9]{1,2} of [0-9]{1,2}')\n",
        "    page3 = re.compile(r'[0-9]{1,2} of [0-9]{1,2}')\n",
        "    marks = re.compile(r'[\\u2612\\uF0B7\\\\x2D\\u2022\\u2713\\x0c●©\\x0f\\x01\\u2009\\u20093▪]')\n",
        "    #roman = re.compile(r' (?:v?iii|i[vx]|x)|x(?:v?ii|vi?|i)?|v(?:i(?:ii|\\.)|\\.)|i(?:ii?\\.|v\\.|[\\.x])|vii ')\n",
        "    questionaire = re.compile(r'1\\-Not important at all|5\\- Very important|2\\- Not important|No opinion|4Important|3Neutral')\n",
        "    single_letter = re.compile(r' [bcdefghjklmnopqrstuvwxyz] ')\n",
        "    spaces = re.compile(r' +')\n",
        "    text = ref.sub(r'',text)\n",
        "    text = ares.sub(r'',text)\n",
        "    text = com.sub(r'',text)\n",
        "    text = bignum.sub(r'',text)\n",
        "    text = parenth.sub(r'',text)\n",
        "    text = email.sub(r'',text)\n",
        "    text = listing.sub(r'',text)\n",
        "    text = page1.sub(r'',text)\n",
        "    text = page2.sub(r'',text)\n",
        "    text = page3.sub(r'',text)\n",
        "    text = marks.sub(r'',text)\n",
        "    #text = roman.sub(r'',text)\n",
        "    text = questionaire.sub(r'',text)\n",
        "    text = single_letter.sub(r'',text)\n",
        "    text = spaces.sub(r' ',text)\n",
        "    return text\n",
        "\n",
        "def clean(df,text_col='Text'):\n",
        "  \"\"\"\n",
        "  Desc:   Pipeline that runs all cleaning functions on DataFrame with texts as\n",
        "          string\n",
        "  Input:  df DataFrame we want to clean\n",
        "          text_col column name we want to clean\n",
        "  Output: df with cleaned text_col column\n",
        "  \"\"\"\n",
        "  df[text_col]=df[text_col].apply(lambda x : my_lower(x))\n",
        "  df[text_col]=df[text_col].apply(lambda x : remove_URL(x))\n",
        "  df[text_col]=df[text_col].apply(lambda x : remove_html(x))\n",
        "  df[text_col]=df[text_col].apply(lambda x : remove_punct(x))\n",
        "  df[text_col]=df[text_col].apply(lambda x : custom_regex(x))\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def customize_stop_words(del_words, en):\n",
        "  \"\"\"\n",
        "  Desc:   Adds custom stopwrods to the original set\n",
        "  Input:  del_words we want to join defalut en stopwords \n",
        "          en model for langugae from spacy (en model)\n",
        "  Output: df with cleaned text_col column\n",
        "  \"\"\"\n",
        "  for l in del_words:\n",
        "    en.vocab[l].is_stop = True\n",
        "    en.Defaults.stop_words.add(l)"
      ],
      "metadata": {
        "id": "vMb6lDEKWqlQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(df,en,text_col='Text'):\n",
        "  \"\"\"\n",
        "  Desc:   Performs Spacy en model tokenization on documents texts and converts \n",
        "          them into spacy.Doc objects.\n",
        "  Input:  df DataFrame to tokenize\n",
        "          en model for langugae from spacy (en model)\n",
        "          text_col column name we want to tokenize         \n",
        "  Output: docs list of Doc objects (lemmatized words)\n",
        "  \"\"\"\n",
        "  tqdm.pandas()\n",
        "  docs = df[text_col].swifter.apply(en)\n",
        "  return docs"
      ],
      "metadata": {
        "id": "wXyVwNpkVigU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "jni16ES5WPPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_len_dist(docs,log_scale=False):\n",
        "  \"\"\"\n",
        "  Desc:   Plots distribution of docs lengths\n",
        "  Input:  docs list of Doc objects \n",
        "          log_scale bool describing if plot should be in log scale        \n",
        "  Output: histplot of docs lengths\n",
        "  \"\"\"\n",
        "  doc_lens = docs.str.len()\n",
        "  doc_lens.hist(log_y=log_scale)"
      ],
      "metadata": {
        "id": "lo68SLWUV-MZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nouns(docs):\n",
        "  \"\"\"\n",
        "  Desc:   Get nouns from docs list\n",
        "  Input:  docs list of Doc objects        \n",
        "  Output: List of nouns that are not stop words from all docs list.\n",
        "  \"\"\"\n",
        "  nouns = [token.text\n",
        "         for doc in docs\n",
        "         for token in doc\n",
        "         if (not token.is_stop and\n",
        "             not token.is_punct and\n",
        "             token.pos_ == \"NOUN\")]\n",
        "  return nouns\n",
        "  \n",
        "def plot_counts(count_obj, names):\n",
        "  \"\"\"\n",
        "  Desc:   Plots counted occurances of object / word\n",
        "  Input:  count_obj DataFrame with name and counted nuber of occurences\n",
        "          names list of column names in count_obj DataFrame       \n",
        "  Output: Plot\n",
        "  \"\"\"\n",
        "  fig = px.bar(count_obj,orientation='h', y=names[0], x=names[1])\n",
        "  fig['layout']['yaxis']['autorange'] = \"reversed\"\n",
        "  fig.update_layout(bargap=0.30, font={'size':10})\n",
        "  return fig\n",
        "\n",
        "def count_texts(texts,colnames=['obj', 'count'],n_obs=30):\n",
        "  obj_freq = Counter(texts)\n",
        "  common_obj = obj_freq.most_common(n_obs)\n",
        "  count_obj = pd.DataFrame(common_obj, columns=colnames)\n",
        "  return count_obj"
      ],
      "metadata": {
        "id": "iXgKJc59WN9z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatized_word_cloud(docs,width=1600,height=800):\n",
        "  \"\"\"\n",
        "  Desc:   Creates word cloud of lemmas from docs list\n",
        "  Input:  docs list of Doc objects \n",
        "          width, height parameters of image       \n",
        "  Output: Plots word cloud\n",
        "  \"\"\"\n",
        "  lemmas = docs.apply(lambda doc: [token.lemma_ for token in doc if not token.is_stop if not token.is_punct if token.is_alpha])\n",
        "  word_counts = Counter(lemmas.sum())\n",
        "  wc = WordCloud(width=1600, height=800)\n",
        "  wc.generate_from_frequencies(frequencies=word_counts)\n",
        "  plt.figure(figsize=(18,14))\n",
        "  plt.imshow(wc)\n",
        "  return word_counts"
      ],
      "metadata": {
        "id": "sLefRopYWZs5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_entities(docs):\n",
        "  \"\"\"\n",
        "  Desc:   Gets all entitites from docs list\n",
        "  Input:  docs list of Doc objects       \n",
        "  Output: list of entities\n",
        "  \"\"\"\n",
        "  entities = [(ent.text, ent.label_)\n",
        "         for doc in docs\n",
        "          for ent in doc.ents]\n",
        "  return entities\n",
        "\n",
        "def unique(list1):\n",
        "  unique_list = []\n",
        "  for x in list1:\n",
        "    if x not in unique_list:\n",
        "      unique_list.append(x)\n",
        "  return unique_list\n",
        "\n",
        "def organisation_like_entitites(docs, uniq=False):\n",
        "  \"\"\"\n",
        "  Desc:   Gets all organisation like entitites from docs list\n",
        "  Input:  docs list of Doc objects\n",
        "          uniq bool describing wheter we want to obtain unique entities only       \n",
        "  Output: list of entities\n",
        "  \"\"\"\n",
        "  entities = get_entities(docs)\n",
        "  if uniq:\n",
        "    entities = unique(entities)\n",
        "  entities_df = pd.DataFrame(entities, columns =['entity','type'])\n",
        "  ls = [\"EVENT\",\"GPE\",\"LAW\",\"NORP\",\"PERSON\",\"ORG\"]\n",
        "  proper_ets = entities_df[entities_df['type'].isin(ls)]\n",
        "  return proper_ets"
      ],
      "metadata": {
        "id": "AFoSk9VdXGbD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tfidf"
      ],
      "metadata": {
        "id": "E96hZm6MXZS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dummy_fun(doc):\n",
        "  \"\"\"\n",
        "  Desc:   required for tfidf_table\n",
        "  \"\"\"\n",
        "  return doc\n",
        "\n",
        "def tfidf_table(texts_df,en,top=10):\n",
        "  \"\"\"\n",
        "  Desc:   With the usage of TfidfVectorizer prepares tdidf DataFrame\n",
        "  Input:  texts_df DataFrame with single column containing texts\n",
        "          en model for langugae from spacy (en model) \n",
        "          top number of how many most important lemmas will be included (importance\n",
        "          by tfidf)\n",
        "  Output: DataFrame where columns are paired as term_X and score_X. This pair\n",
        "          describes importance of lemmas for Xth document\n",
        "  \"\"\"\n",
        "  stopwords = en.Defaults.stop_words\n",
        "\n",
        "  vectorizer = TfidfVectorizer(stop_words=stopwords, use_idf=True, norm=None)\n",
        "  #vectorizer = TfidfVectorizer(analyzer='word',tokenizer=dummy_fun,preprocessor=dummy_fun,token_pattern=None) \n",
        "  transformed_documents = vectorizer.fit_transform(texts_df)\n",
        "  #transformed_documents = vectorizer.fit_transform(docs)\n",
        "\n",
        "  transformed_documents_as_array = transformed_documents.toarray()\n",
        "  output_filenames = [range(len(transformed_documents_as_array))]\n",
        "\n",
        "  docs_as_dfs = []\n",
        "  for counter, doc in enumerate(transformed_documents_as_array):\n",
        "      tf_idf_tuples = list(zip(vectorizer.get_feature_names(), doc))\n",
        "      one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
        "      docs_as_dfs.append(one_doc_as_df)\n",
        "  top_tfidf = docs_as_dfs[0][:top]\n",
        "  for i in range(len(docs_as_dfs)-1):\n",
        "    top_tfidf = pd.concat([top_tfidf, docs_as_dfs[i+1][:10]], axis=1)\n",
        "\n",
        "  tfidf_names = []\n",
        "  for i in range(len(docs_as_dfs)):\n",
        "    tfidf_names.append(\"term_\"+str(i))\n",
        "    tfidf_names.append(\"score_\"+str(i))\n",
        "  top_tfidf.columns = tfidf_names\n",
        "\n",
        "  return top_tfidf\n",
        "\n",
        "def counts_tfidf(top_tfidf):\n",
        "  \"\"\"\n",
        "  Desc:   Counts in how many documents the single lemma was in top lemmas\n",
        "  Input:  top_tfidf DataFrame created by tfidf_table()\n",
        "  Output: list with counted occurences of the lemma in tfidfs top lemmas\n",
        "  \"\"\"\n",
        "  terms_tfidf = top_tfidf.loc[:, ::2]\n",
        "  terms_list = []\n",
        "  for i in range(len(terms_tfidf.columns)):\n",
        "    for j in range(len(terms_tfidf)):\n",
        "      terms_list.append(terms_tfidf.iloc[j,i])\n",
        "  terms_freq = Counter(terms_list)\n",
        "  common_terms = terms_freq.most_common(40)\n",
        "  count_terms = pd.DataFrame(common_terms, columns=['term', 'count'])\n",
        "  return count_terms"
      ],
      "metadata": {
        "id": "b2RGfc9-XV91"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ngrams"
      ],
      "metadata": {
        "id": "bNrluONLX_We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_ngram(texts_df, stopwords, n=None, m=None, num = 30):\n",
        "  \"\"\"\n",
        "  Desc:   Prepares top ngrams after removing the stop words from text given in\n",
        "          texts_df list of strings\n",
        "  Input:  texts_df DataFrame with single column containing texts\n",
        "          stopwords that won't be included in the ngrams\n",
        "          n,m sets range of ngram, f.ex. n=2,m=4 says that ngrams with 2,3 or 4\n",
        "          words inside them will be searched\n",
        "  Output: list of most common ngrams in given range\n",
        "  \"\"\"  \n",
        "  vec = CountVectorizer(stop_words = stopwords, ngram_range=(n, m)).fit(texts_df)\n",
        "  bag_of_words = vec.transform(texts_df)\n",
        "  sum_words = bag_of_words.sum(axis=0) \n",
        "  words_freq = [(word, sum_words[0, idx]) \n",
        "                for word, idx in vec.vocabulary_.items()]\n",
        "  words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "  return words_freq[:num]\n",
        "    \n",
        "def plot_ngram(top_ngrams):\n",
        "  \"\"\"\n",
        "  Desc:   Prepares barplot for top ngrams\n",
        "  Input:  top_ngrams list of most common ngrams\n",
        "  \"\"\"\n",
        "  x,y=map(list,zip(*top_ngrams))\n",
        "  fig= pyplot.subplots(figsize=(15,15))\n",
        "  sns.barplot(x=y,y=x)"
      ],
      "metadata": {
        "id": "btsGuqDoX-vv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noun Chunks"
      ],
      "metadata": {
        "id": "mV8NszrsYEVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_noun_chunks(docs, stopwords = False):\n",
        "  \"\"\"\n",
        "  Desc:   Prepares list of noun chunks as strings present in all docs\n",
        "  Input:  docs list of Doc objects \n",
        "          stopwords bool describving if we want to remove stopwords or not\n",
        "  Output: list of noun chunks as strings\n",
        "  \"\"\"\n",
        "  noun_chunks = []\n",
        "  for doc in docs:\n",
        "    for chunk in doc.noun_chunks:\n",
        "      if stopwords:\n",
        "        stop = True\n",
        "        for w in chunk:\n",
        "          if w.is_stop:\n",
        "            stop = False\n",
        "        if stop:\n",
        "          noun_chunks.append(chunk.text)\n",
        "      else:\n",
        "        noun_chunks.append(chunk.text)\n",
        "  return noun_chunks"
      ],
      "metadata": {
        "id": "7jU_lj1vYF2e"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chunks(docs, stopwords = False):\n",
        "  \"\"\"\n",
        "  Desc:   Prepares list of noun chunks present in all docs\n",
        "  Input:  docs list of Doc objects \n",
        "          stopwords bool describving if we want to remove stopwords or not\n",
        "  Output: list of noun chunks\n",
        "  \"\"\"\n",
        "  chunks = list()\n",
        "  if stopwords:\n",
        "    for doc in docs:\n",
        "      ok_chunks = list()\n",
        "      for chunk in doc.noun_chunks:\n",
        "        stop = True\n",
        "        for w in chunk:\n",
        "          if w.is_stop:\n",
        "            stop = False\n",
        "        if stop:\n",
        "          ok_chunks = ok_chunks + chunk\n",
        "      chunks = chunks + list(ok_chunks)\n",
        "  else:\n",
        "    for doc in docs:\n",
        "      chunks = chunks + list(doc.noun_chunks)\n",
        "  return chunks\n",
        "\n",
        "def top_chunk_parents(chunks,count_chunks,n_chunks=10,n_parents=5):\n",
        "  \"\"\"\n",
        "  Desc:   Prepares list of noun chunks present in all docs\n",
        "  Input:  chunks list of noun chunks\n",
        "          count_chunks counted list of noun chunks as strings (from get_noun_chunks)\n",
        "          n_chunks number of most occuring chunks we want to consider\n",
        "          n_parents number of parents we are looking for each chunk\n",
        "  Output: list of DataFrames with 3 columns: chunk, parent, count\n",
        "  \"\"\"\n",
        "  chunk_parents = []\n",
        "  for chunk in chunks:\n",
        "    if chunk.text in (list(count_chunks[\"chunk\"][:n_chunks])):\n",
        "      chunk_parents.append((chunk.text,chunk.root.head.text))\n",
        "  ch_p_df = pd.DataFrame(chunk_parents, columns =['chunk','parent'])\n",
        "  count_ch_p = ch_p_df.value_counts()\n",
        "  count_ch_p = count_ch_p.reset_index()\n",
        "  count_ch_p.columns =[\"chunk\",\"parent\",\"count\"]\n",
        "  most_common_parents = []\n",
        "  for el in list(count_chunks[\"chunk\"][:n_chunks]):\n",
        "    p = count_ch_p[count_ch_p[\"chunk\"]== el][:n_parents]\n",
        "    most_common_parents.append(p)\n",
        "  return most_common_parents\n",
        "\n",
        "def count_texts(texts,colnames=['obj', 'count'],n_obs=30):\n",
        "  obj_freq = Counter(texts)\n",
        "  common_obj = obj_freq.most_common(n_obs)\n",
        "  count_obj = pd.DataFrame(common_obj, columns=colnames)\n",
        "  return count_obj"
      ],
      "metadata": {
        "id": "ohvL0fsuYJLI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_chunks_parents(most_common_parents):\n",
        "  \"\"\"\n",
        "  Desc:   Prepares visualization for top_chunk_parents function\n",
        "  Input:  most_common_parents - output of top_chunk_parents function\n",
        "  \"\"\"\n",
        "  figure(figsize=(20, 12), dpi=100)\n",
        "  plt.figure(1)\n",
        "  for i in range(9):\n",
        "    plt.subplot(331+i)\n",
        "    plt.bar(most_common_parents[i][\"parent\"], most_common_parents[i][\"count\"])\n",
        "    plt.title(most_common_parents[i][\"chunk\"].iloc[0])"
      ],
      "metadata": {
        "id": "NhrwDMkBY0xW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_frequency(docs, n_top_chunks = 10, stopwords = False):\n",
        "  \"\"\"\n",
        "  Desc:   Prepares a DataFrame with 4 columns: Chunk,Count,ChunkFrequency and \n",
        "          Percent. ChunkFrequency informs us in how many documents given noun \n",
        "          chunk existed and Percent gives us the percentage of this event in terms\n",
        "          of total number of documents\n",
        "  Input:  docs list of Doc objects \n",
        "          n_top_chunks number of most occuring chunks we want to consider\n",
        "          stopwords bool describving if we want to remove stopwords or not\n",
        "  Output: DataFrame with 4 columns: Chunk,Count,ChunkFrequency and \n",
        "          Percent.\n",
        "  \"\"\"\n",
        "  freq_chunk = []\n",
        "  for doc in docs:\n",
        "    n_chunks = []\n",
        "    for chunk in doc.noun_chunks:\n",
        "      if stopwords:\n",
        "        stop = True\n",
        "        for w in chunk:\n",
        "          if w.is_stop:\n",
        "            stop = False\n",
        "        if stop:\n",
        "          n_chunks.append(chunk.text)\n",
        "      else:\n",
        "        n_chunks.append(chunk.text)\n",
        "    freq_chunk.append(n_chunks)\n",
        "\n",
        "  noun_chunks = get_noun_chunks(docs, stopwords)\n",
        "  noun_chunks = list(filter(lambda x: len(x.split()) > 1, noun_chunks))\n",
        "  count_chunks = count_texts(noun_chunks,['chunk', 'count'],n_top_chunks)\n",
        "  common_chunks = count_chunks['chunk']\n",
        "  count_chunks = count_chunks['count']\n",
        "\n",
        "  count_chunk = []\n",
        "  for ch in common_chunks:\n",
        "    x=0\n",
        "    for i in range(len(freq_chunk)):\n",
        "      if ch in freq_chunk[i]:\n",
        "        x+=1\n",
        "    count_chunk.append(x)\n",
        "  \n",
        "  ch_name = []\n",
        "  ch_count = []\n",
        "  for ch in common_chunks:\n",
        "    ch_name.append(ch)\n",
        "  for ch in count_chunks:\n",
        "    ch_count.append(ch)\n",
        "  \n",
        "  chunk_count_df = pd.DataFrame(ch_name)\n",
        "  chunk_count_df[\"Count\"] = ch_count\n",
        "  chunk_count_df[\"CF\"] = count_chunk\n",
        "  chunk_count_df[\"Percent\"] = [x / len(docs) for x in count_chunk]\n",
        "  chunk_count_df.columns = [\"Chunk\",\"Count\",\"ChunkFrequency\", \"Percent\"]\n",
        "\n",
        "  return chunk_count_df"
      ],
      "metadata": {
        "id": "3bu1Ri4HbAuy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_count_chunks(chunk_count_df):\n",
        "  \"\"\"\n",
        "  Desc:   Prepares a visualization for output of chunk_frequency() output\n",
        "  Input:  DataFrame with 4 columns: Chunk,Count,ChunkFrequency and \n",
        "          Percent.\n",
        "  \"\"\"\n",
        "  labels = list(chunk_count_df[\"Chunk\"])\n",
        "\n",
        "  x = np.arange(len(chunk_count_df[\"Chunk\"]))  # the label locations\n",
        "  width = 0.35  # the width of the bars\n",
        "  fig, ax = plt.subplots(figsize=(20, 12))\n",
        "  rects1 = ax.bar(x - width/2, chunk_count_df[\"Count\"], width, label='Count')\n",
        "  rects2 = ax.bar(x + width/2, chunk_count_df[\"ChunkFrequency\"], width, label='ChunkFrequency')\n",
        "\n",
        "  ax.set_xticks(x, labels)\n",
        "  ax.legend()\n",
        "  fig.tight_layout()\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "McTXUDLzeBHe"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Textrank"
      ],
      "metadata": {
        "id": "6hbLSOJpfKTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def textrank(docs, n_of_keyterms = 40):\n",
        "  \"\"\"\n",
        "  Desc:   Performs Textrank and provides a visualization for it\n",
        "  Input:  docs list of Doc objects\n",
        "          n_of_keyterms number of keyterms returned by the analysis\n",
        "  Output: Visualization of the textrank\n",
        "  \"\"\"\n",
        "  keyterms = []\n",
        "  for doc in docs:\n",
        "    keyterms.append(textacy.extract.keyterms.textrank(doc))\n",
        "  keyterms_list = []\n",
        "  for i in range(len(docs)):\n",
        "    keyterms_df = pd.DataFrame.from_dict(keyterms[i])\n",
        "    for j in range(len(keyterms_df[0])):\n",
        "      keyterms_list.append(keyterms_df[0][j])\n",
        "  keyterms_freq = Counter(keyterms_list)\n",
        "  common_keyterms = keyterms_freq.most_common(n_of_keyterms)\n",
        "  count_keyterms = pd.DataFrame(common_keyterms, columns=['keyterm', 'count'])\n",
        "  fig = px.bar(count_keyterms,orientation='h', y='keyterm', x='count')\n",
        "\n",
        "  fig['layout']['yaxis']['autorange'] = \"reversed\"\n",
        "  fig.update_layout(bargap=0.30, font={'size':10})\n",
        "  fig"
      ],
      "metadata": {
        "id": "Pj6XXohMfNbG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment"
      ],
      "metadata": {
        "id": "BTVu8v0AitBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_sentiment(docs):\n",
        "  \"\"\"\n",
        "  Desc:   Performs sentiment analysis involving poalrity and subjectibity measures\n",
        "  Input:  docs list of Doc objects\n",
        "  Output: DataFrame with 2 columns: polarity and subjectivity\n",
        "  \"\"\"\n",
        "  sentiment_info = []\n",
        "  for doc in docs:\n",
        "    sentiment = (doc._.blob.polarity, doc._.blob.subjectivity)\n",
        "    sentiment_info.append(sentiment)\n",
        "  sentiment_df = pd.DataFrame.from_dict(sentiment_info)\n",
        "  sentiment_df.columns=[\"polarity\",\"subjectivity\"]\n",
        "  return sentiment_df"
      ],
      "metadata": {
        "id": "cdWXtPhnivAn"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}