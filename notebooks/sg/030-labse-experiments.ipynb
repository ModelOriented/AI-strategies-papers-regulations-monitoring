{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11f5663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-30 11:57:45.086684: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-07-30 11:57:45.086705: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text  # Needed for loading universal-sentence-encoder-cmlm/multilingual-preprocess\n",
    "import numpy as np\n",
    "\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d115bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(embeds):\n",
    "  norms = np.linalg.norm(embeds, 2, axis=1, keepdims=True)\n",
    "  return embeds/norms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1898aeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-30 11:58:36.133746: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-07-30 11:58:36.169838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-30 11:58:36.170220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:0b:00.0 name: GeForce RTX 2080 SUPER computeCapability: 7.5\n",
      "coreClock: 1.845GHz coreCount: 48 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 462.00GiB/s\n",
      "2021-07-30 11:58:36.170282: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-07-30 11:58:36.170332: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2021-07-30 11:58:36.170366: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2021-07-30 11:58:36.170400: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2021-07-30 11:58:36.170432: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2021-07-30 11:58:36.170463: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2021-07-30 11:58:36.170494: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2021-07-30 11:58:36.170525: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-07-30 11:58:36.170531: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-07-30 11:58:36.170865: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-30 11:58:36.171293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-07-30 11:58:36.171300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n",
      "2021-07-30 11:58:37.348519: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-30 11:58:37.348940: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3593180000 Hz\n"
     ]
    }
   ],
   "source": [
    "preprocessor = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/google/LaBSE/2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c015d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = tf.constant([\"dog\", \"Puppies are nice.\", \"I enjoy taking long walks along the beach with my dog.\"])\n",
    "italian_sentences = tf.constant([\"cane\", \"I cuccioli sono carini.\", \"Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane.\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f506e7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedd_sents(sents):\n",
    "    return normalization(encoder(preprocessor(tf.constant(sents)))[\"default\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bcc1c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_embeds = encoder(preprocessor(english_sentences))[\"default\"]\n",
    "italian_embeds = encoder(preprocessor(italian_sentences))[\"default\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed81e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_embeds = normalization(english_embeds)\n",
    "italian_embeds = normalization(italian_embeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e18ea2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6319289  0.30619788 0.4429748 ]\n",
      " [0.11652693 0.8596667  0.35940555]\n",
      " [0.14803988 0.32447952 0.9542651 ]]\n"
     ]
    }
   ],
   "source": [
    "print (np.matmul(english_embeds, np.transpose(italian_embeds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88fc06d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a0b5f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for p in glob.glob('../../data/ethics-ai-table/*.txt'):\n",
    "    with open(p, 'r') as f:\n",
    "        t = f.read()\n",
    "        texts.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22cbe5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nOur Goals\\n1\\nDevelop and share best practices\\n\\nSupport research, discussions, identification, sharing, and recommendation of best practices in the research, development, testing, and fielding of AI technologies. Address such areas as fairness and inclusivity, explanation and transparency, security and privacy, values and ethics, collaboration between people and AI systems, interoperability of systems, and of the trustworthiness, reliability, containment, safety, and robustness of the technology.\\n2\\nAdvance public understanding\\n\\nAdvance wider public understanding and awareness of AI by sharing insights into AI’s core technologies, potential benefits – and costs. We will act as a trusted experts on AI for society and their leaders, and will work to increase public understanding of how AI is progressing.\\n3\\nProvide an open and inclusive platform for discussion & engagement\\n\\nCreate and support opportunities for AI researchers and key stakeholders, including people in technology, law, policy, government, civil liberties, and the greater public, to communicate directly and openly with each other about relevant issues to AI and its influences on people and society. Ensure that key stakeholders have the knowledge, resources, and overall capacity to participate fully.\\n4\\nIdentify and foster aspirational efforts in AI for socially beneficial purposes\\n\\nSeek out, support, celebrate, and highlight aspirational efforts in AI for socially benevolent applications. Identify areas of untapped opportunity, including promising technologies and applications not being explored by academia and industry R&D.\\nOur Work (Thematic Pillars)\\n\\n    1\\n    Safety-Critical AI\\n\\n    Advances in AI have the potential to improve outcomes, enhance quality, and reduce costs in such safety-critical areas as healthcare and transportation. Effective and careful applications of pattern recognition, automated decision making, and robotic systems show promise for enhancing the quality of life and preventing thousands of needless deaths.\\n\\n    However, where AI tools are used to supplement or replace human decision-making, we must be sure that they are safe, trustworthy, and aligned with the ethics and preferences of people who are influenced by their actions.\\n\\n    We will pursue studies and best practices around the fielding of AI in safety-critical application areas.\\n    2\\n    Fair, Transparent, and Accountable AI\\n\\n    AI has the potential to provide societal value by recognizing patterns and drawing inferences from large amounts of data. Data can be harnessed to develop useful diagnostic systems and recommendation engines, and to support people in making breakthroughs in such areas as biomedicine, public health, safety, criminal justice, education, and sustainability.\\n\\n    While such results promise to provide real benefits, we need to be sensitive to the possibility that there are hidden assumptions and biases in data, and therefore in the systems built from that data — in addition to a wide range of other system choices which can be impacted by biases, assumptions, and limits. This can lead to actions and recommendations that replicate those biases, and have serious blind spots.\\n\\n    Researchers, officials, and the public should be sensitive to these possibilities and we should seek to develop methods that detect and correct those errors and biases, not replicate them. We also need to work to develop systems that can explain the rationale for inferences.\\n\\n    We will pursue opportunities to develop best practices around the development and fielding of fair, explainable, and accountable AI systems.\\n    3\\n    AI, Labor, and the Economy\\n\\n    AI advances will undoubtedly have multiple influences on the distribution of jobs and nature of work. While advances promise to inject great value into the economy, they can also be the source of disruptions as new kinds of work are created and other types of work become less needed due to automation.\\n\\n    Discussions are rising on the best approaches to minimizing potential disruptions, making sure that the fruits of AI advances are widely shared and competition and innovation are encouraged and not stifled. We seek to study and understand best paths forward, and play a role in this discussion.\\n    4\\n    Collaborations Between People and AI Systems\\n\\n    A promising area of AI is the design of systems that augment the perception, cognition, and problem-solving abilities of people. Examples include the use of AI technologies to help physicians make more timely and accurate diagnoses and assistance provided to drivers of cars to help them to avoid dangerous situations and crashes.\\n\\n    Opportunities for R&D and for the development of best practices on AI-human collaboration include methods that provide people with clarity about the understandings and confidence that AI systems have about situations, means for coordinating human and AI contributions to problem solving, and enabling AI systems to work with people to resolve uncertainties about human goals.\\n    5\\n    Social and Societal Influences of AI\\n\\n    AI advances will touch people and society in numerous ways, including potential influences on privacy, democracy, criminal justice, and human rights. For example, while technologies that personalize information and that assist people with recommendations can provide people with valuable assistance, they could also inadvertently or deliberately manipulate people and influence opinions.\\n\\n    We seek to promote thoughtful collaboration and open dialogue about the potential subtle and salient influences of AI on people and society.\\n    6\\n    AI and Social Good\\n\\n    AI offers great potential for promoting the public good, for example in the realms of education, housing, public health, and sustainability. We see great value in collaborating with public and private organizations, including academia, scientific societies, NGOs, social entrepreneurs, and interested private citizens to promote discussions and catalyze efforts to address society’s most pressing challenges.\\n\\n    Some of these projects may address deep societal challenges and will be moonshots – ambitious big bets that could have far-reaching impacts. Others may be creative ideas that could quickly produce positive results by harnessing AI advances.\\n\\nHow we're doing it:\\n\\n    Engaging Experts\\n\\n    The regular engagement of experts across multiple disciplines (including but not limited to psychology, philosophy, economics, finance, sociology, public policy, and law) to discuss and provide guidance on emerging issues related to the impact of AI on society.\\n    Engaging Stakeholders\\n\\n    The engagement of AI users and developers, as well as representatives of industry sectors that may be impacted by AI (such as healthcare, financial services, transportation, commerce, manufacturing, telecommunications, and media) to support best practices in the research, development, and use of AI technology within specific domains.\\n    Enabling Study\\n\\n    The design, execution, and financial support of objective third-party studies on best practices for the ethics, safety, fairness, inclusiveness, trust, and robustness for AI research, applications, and services. The identification and celebration of important work in these fields. The support of aspirational projects in AI that would greatly benefit people and society.\\n    Learning Materials\\n\\n    The development of informational materials on the current and future likely trajectories of research and development in core AI and related disciplines.\\n\\n\\n\",\n",
       " '\\nOpenAI Charter\\n\\nWe’re releasing a charter that describes the principles we use to execute on OpenAI’s mission. This document reflects the strategy we’ve refined over the past two years, including feedback from many people internal and external to OpenAI. The timeline to AGI remains uncertain, but our charter will guide us in acting in the best interests of humanity throughout its development.\\nApril 9, 2018\\n5 minute read\\nEnglish\\n中文\\n\\nOpenAI’s mission is to ensure that artificial general intelligence (AGI)—by which we mean highly autonomous systems that outperform humans at most economically valuable work—benefits all of humanity. We will attempt to directly build safe and beneficial AGI, but will also consider our mission fulfilled if our work aids others to achieve this outcome. To that end, we commit to the following principles:\\nBroadly Distributed Benefits\\n\\n    We commit to use any influence we obtain over AGI’s deployment to ensure it is used for the benefit of all, and to avoid enabling uses of AI or AGI that harm humanity or unduly concentrate power.\\n    Our primary fiduciary duty is to humanity. We anticipate needing to marshal substantial resources to fulfill our mission, but will always diligently act to minimize conflicts of interest among our employees and stakeholders that could compromise broad benefit.\\n\\nLong-Term Safety\\n\\n    We are committed to doing the research required to make AGI safe, and to driving the broad adoption of such research across the AI community.\\n    We are concerned about late-stage AGI development becoming a competitive race without time for adequate safety precautions. Therefore, if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project. We will work out specifics in case-by-case agreements, but a typical triggering condition might be “a better-than-even chance of success in the next two years.”\\n\\nTechnical Leadership\\n\\n    To be effective at addressing AGI’s impact on society, OpenAI must be on the cutting edge of AI capabilities—policy and safety advocacy alone would be insufficient.\\n    We believe that AI will have broad societal impact before AGI, and we’ll strive to lead in those areas that are directly aligned with our mission and expertise.\\n\\nCooperative Orientation\\n\\n    We will actively cooperate with other research and policy institutions; we seek to create a global community working together to address AGI’s global challenges.\\n    We are committed to providing public goods that help society navigate the path to AGI. Today this includes publishing most of our AI research, but we expect that safety and security concerns will reduce our traditional publishing in the future, while increasing the importance of sharing safety, policy, and standards research.\\n\\n\\n',\n",
       " '\\nResponsible AI\\n\\nWe are committed to the advancement of AI driven by ethical principles that put people first.\\nPlay video on our approach\\n\\n \\n\\n \\nMicrosoft AI principles\\n\\nWe put our responsible AI principles into practice through the Office of Responsible AI (ORA) and the AI, Ethics, and Effects in Engineering and Research (Aether) Committee. The Aether Committee advises our leadership on the challenges and opportunities presented by AI innovations. ORA sets our rules and governance processes, working closely with teams across the company to enable the effort.\\n\\nLearn more about our approach\\nFairness\\n\\nAI systems should treat all people fairly\\nPlay video on fairness\\nReliability & Safety\\n\\nAI systems should perform reliably and safely\\nPlay video on reliability\\nPrivacy & Security\\n\\nAI systems should be secure and respect privacy\\nPlay video on privacy\\nInclusiveness\\n\\nAI systems should empower everyone and engage people\\nPlay video on inclusiveness\\nTransparency\\n\\nAI systems should be understandable\\nPlay video on transparency\\nAccountability\\n\\nPeople should be accountable for AI systems\\nPlay video on accountability\\nOur approach\\nWoman and man look at notes on a glass wall.\\nInnovating responsibly\\n\\nWe are putting our principles into practice by taking a people-centered approach to the research, development, and deployment of AI. To achieve this, we embrace diverse perspectives, continuous learning, and agile responsiveness as AI technology evolves.\\nA woman in discussion with coworkers leans over a conference table.\\nEmpowering others\\n\\nWe are helping organizations cultivate a responsible AI-ready culture throughout their businesses and put principles into place from implementation to governance with practices, tools, and technologies built on multidisciplinary research, shared learning, and leading innovation.\\nA doctor checks a man’s heart rate on a bench\\nFostering positive impact\\n\\nWe are committed to ensuring AI technology has a lasting positive impact for everyone by helping to shape public policy, contributing to industry working groups, and empowering those working to address society’s greatest challenges through our AI for Good initiative.\\nPut responsible AI into action\\n\\nDiscover resources to help you implement responsible AI.\\n\\nEscalator with large buildings in the background\\nEstablish a responsible AI strategy\\n\\nLearn how to develop your own responsible AI strategy and principles based on the values of your organization.\\nGet started at AI Business School\\n2 men look at a large monitor together.\\nDevelop AI responsibly\\n\\nGet access to tools, guidelines, and additional resources that will help you create a responsible AI solution.\\nExplore responsible AI resources\\nPartner and customer stories\\n\\nLearn how others are innovating responsibly across industries.\\n\\nState Farm\\nTelefónica\\nTD Bank Group\\nHands holding a credit card and using a laptop.\\nState Farm\\n\\nState Farm® and its affiliates are the largest providers of auto and home insurance in the U.S. To responsibly maintain and amplify its services, State Farm has established a governance system assigning accountability for AI and overseeing the development of AI solutions that benefit customers.\\nLearn about State Farm and AI\\nPerspectives on responsible AI\\n\\nLearn more from experts across Microsoft working to advance our understanding, practices, and technology for responsible AI.\\n\\nCollage of faces showing AI face recognition overlays.\\nProgress on regulating facial recognition\\n\\nMicrosoft President Brad Smith discusses the importance of Washington Governor Jay Inslee signing landmark facial recognition legislation that the state legislature passed on March 12.\\nLearn about the new legislation\\nAbstract structure with the sky in the background\\nThe future as AI-human partnership\\n\\nMicrosoft Chief Scientific Officer Dr. Eric Horvitz shares his thoughts on the evolving relationship between humans and machines, and how AI trends have more to do with creating synergies than competition with humans.\\nLearn about AI-human partnership\\nRows of desks in a classroom.\\nResponsible AI starts with higher education\\n\\nMicrosoft and Seattle University are working together to develop curricula and best practices to support responsible AI.\\nCheck out the collaboration\\nAdvancing trustworthy innovation\\n\\nWe are leading the industry in the advancement of responsible AI through our work with partners and researchers.\\n\\nStreaks of colorful light going across the screen in a wavy pattern\\nResearch and innovation for responsible AI\\n\\nMicrosoft is collaborating with researchers and academics around the globe in an effort to advance responsible AI practices and technologies. Visit our research collection to get an overview of this work.\\nLearn about responsible AI research\\nTop-down view of several pairs of people shaking hands on colorful cement with designs on it\\nCollaborating to develop industry-leading best practices\\n\\nMicrosoft co-founded The Partnership on AI, a working group that brings together industry leaders, academics, nonprofits, and specialists to develop best practices, and provides an open platform for discussion and engagement around AI’s impact on people and society.\\nLearn about The Partnership on AI\\n\\n',\n",
       " \" Ethics & Society team\\nExploring the real-world impacts of AI\\n\\nEthics & Society\\nAs scientists and practitioners, we take responsibility for investigating the impacts of our work.\\nOverview\\n\\nSecuring safe, accountable, and socially beneficial technology cannot be an afterthought. With the right focus on ethical standards and safety, we have better chances of finding AI’s potential benefits. By researching the ethical and social questions involving AI, we ensure these topics remain at the heart of everything we do.\\nSocial purpose\\n\\nWe start from the belief that AI should be used for socially beneficial purposes and always remain under meaningful human control. Understanding what this means in practice is essential.  \\n\\nFinding ways to involve the broader society in our work is fundamental to our mission, so partnerships with others in the field of AI ethics is a crucial element of our approach.\\n\\nWe embrace scientific values like transparency, freedom of thought, and the equality of access, and we deeply respect the independence and academic integrity of our researchers and partners.\\n\\nOur work\\nQuestions about AI extend far beyond its technology. Through our partnerships, we’ve created public lectures, forums, and resources to better understand the societal impacts of AI. Below are some of our recent highlights.\\nLaunching public lectures\\n\\nWe partnered with the Royal Society on a free public lecture and panel series, You & AI. These lectures, featuring experts like Kate Crawford and Joseph Stiglitz, explored AI’s capabilities, future directions, and potential societal effects. Each lecture was recorded and is available to watch online.\\n\\nEngaging citizens directly\\n\\nTogether with the RSA, we created the Forum for Ethical AI, a public engagement programme for discussing the use of automated decision-making tools. During this forum, citizen participants developed a critical framework for addressing transparency, accountability, and accessibility of AI technology.\\n\\nConvening experts\\n\\nIn partnership with Princeton University, we organised a workshop to explore how criminal justice systems use AI technology. We brought together technologists and advocates to discuss solutions and create resources, directly informed by affected communities, which explore the harm that can be caused by predictive tools.\\n\\nThemes\\n\\nWe want to promote research that ensures AI works for all. Our research themes are designed to reflect the key ethical challenges that exist for us and the wider AI community. We undertake research and collaborations in each of these areas, determined by the urgent challenges ahead.\\nPrivacy, transparency, and fairness\\n\\nAI systems can use large-scale and sometimes sensitive datasets, such as medical or criminal justice records. This raises important questions about protecting people’s privacy and ensuring that they understand how their data is used. Also, the data used for training automated decision-making systems can contain biases, creating systems that might discriminate against certain groups of people. \\n\\n    How do concepts such as consent and ownership relate to using data in AI systems? \\n    What can AI researchers do to detect and minimise the effects of bias?\\n    What policies and tools allow meaningful audits of AI systems and their data?\\n\\nFellows\\n\\nOur fellows are independent advisors that help provide critical feedback and guidance. These research fellows not only bring expertise but also their values and capacity for asking challenging questions. Engaging with world-class philosophers, economists, and practitioners help us better understand the implications of AI, keeping us focused on questions that matter.\\nPhoto of Nick Bostrom\\nNick Bostrom\\n\\nProfessor at the University of Oxford, Director of the Future of Humanity Institute and the Governance of Artificial Intelligence Program\\nPhoto of Diane Coyle\\nProfessor Diane Coyle\\n\\nBennett Professor of Public Policy, University of Cambridge\\nPhoto of Edward Felten\\nProfessor Edward W Felten\\n\\nProfessor of Computer Science and Public Affairs, Founding Director of Princeton's Center for Information Technology Policy\\nPhoto of James Manyika\\nJames Manyika\\n\\nSenior Partner at McKinsey & Company and Chair of the McKinsey Global Institute\\nPhoto of Jeffrey Sachs\\nProfessor Jeffrey D Sachs\\n\\nProfessor of Economics, Director of the Center for Sustainable Development at Columbia University and Senior UN Advisor\\nPartners\\n\\nCollaboration, diversity of thought, and meaningful public engagement are key if we are to develop and apply AI for maximum benefit. The Ethics & Society team works with a variety of partners in an effort to support and learn from the broadest possible viewpoints, creating space for interdisciplinary collaboration that can approach complex challenges in creative ways. We will always be open about who we work with and what projects we fund. All of our research grants will be unrestricted and we will never attempt to influence or pre-determine the outcome of studies we commission. When we collaborate or co-publish with external researchers, we will disclose whether they have received funding from us.\\n\\n\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a076e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7ecf5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = list(en(texts[0]).sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bfd99d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [str(s) for s in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d3184c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_sents =embedd_sents(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98bdc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['privacy protection',\n",
    "'fairness',\n",
    "'transparency, openness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56773a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_tergets = embedd_sents(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef8efaca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d3e5f556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31,  1, 19])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5921cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_best_sents(e_targets, e_sents, sents, targets):\n",
    "    r = np.matmul(e_tergets, np.transpose(e_sents))\n",
    "    for j, (i, target) in enumerate(zip(list(np.argmax(r, axis=1)), targets)):\n",
    "        print(target)\n",
    "        print(\"SCORE:\", r[j][i])\n",
    "        print(\"BEST HIT:\")\n",
    "        print(sents[i])\n",
    "        print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "08714d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "privacy protection\n",
      "SCORE: 0.28070253\n",
      "BEST HIT:\n",
      "\n",
      "    5\n",
      "    Social and Societal Influences of AI\n",
      "\n",
      "    AI advances will touch people and society in numerous ways, including potential influences on privacy, democracy, criminal justice, and human rights.\n",
      "------------------------------\n",
      "fairness\n",
      "SCORE: 0.25957346\n",
      "BEST HIT:\n",
      "Address such areas as fairness and inclusivity, explanation and transparency, security and privacy, values and ethics, collaboration between people and AI systems, interoperability of systems, and of the trustworthiness, reliability, containment, safety, and robustness of the technology.\n",
      "------------------------------\n",
      "transparency, openness\n",
      "SCORE: 0.32075033\n",
      "BEST HIT:\n",
      "\n",
      "\n",
      "    Researchers, officials, and the public should be sensitive to these possibilities and we should seek to develop methods that detect and correct those errors and biases, not replicate them.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_best_sents(e_tergets, e_sents, sents, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c161a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
