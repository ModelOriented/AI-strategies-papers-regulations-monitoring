{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import shutil\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "from typing import Tuple, List\n",
    "\n",
    "import en_core_web_lg\n",
    "import pandas as pd\n",
    "import spacy.tokens.doc\n",
    "import typer\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from mars.definition_extraction import DeftCorpusLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_to_spacy3(frame: pd.DataFrame, cats: list) -> List[Tuple]:\n",
    "    ret = []\n",
    "    sentences = frame['Sentence'].reset_index(drop=True)\n",
    "    for i in range(len(sentences)):\n",
    "        if cats[i]['DEFINITION'] is True:\n",
    "            cat = 1\n",
    "        else:\n",
    "            cat = 0\n",
    "\n",
    "        ret.append((sentences[i], cat))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def create_from_wiki(path='../../mars/definition_extraction/wcl_datasets_v1.2/wikipedia/', files=None) -> List[Tuple]:\n",
    "    if files is None:\n",
    "        files = {'wiki_bad.txt': 0, 'wiki_good.txt': 1}\n",
    "\n",
    "    file_sentences = {}\n",
    "\n",
    "    for f in files.keys():\n",
    "        filename = os.path.join(path, f)\n",
    "        defs = []\n",
    "        with open(filename, 'rb') as file:\n",
    "            lines = file.readlines()\n",
    "            lines = np.array([line.rstrip() for line in lines])\n",
    "\n",
    "        for line in lines[1::2]:\n",
    "            defs.append(str(line)[2:].split(':')[0])\n",
    "\n",
    "        lines = lines[::2]\n",
    "        for i, line in enumerate(lines):\n",
    "            lines[i] = str(line)[4:-2]\n",
    "        lines = lines.astype(str)\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            lines[i] = lines[i].replace('TARGET', defs[i])\n",
    "\n",
    "        lines = list(lines)\n",
    "        lines = [(str(line), int(files[f])) for line in lines]\n",
    "\n",
    "        file_sentences[f] = lines\n",
    "\n",
    "    file_sentences = list(file_sentences.values())[0] + list(file_sentences.values())[1]\n",
    "    return file_sentences\n",
    "\n",
    "\n",
    "def filter_out(sentences: List[Tuple], max_length: int) -> List[Tuple]:\n",
    "    res = []\n",
    "    for x in sentences:\n",
    "        if len(x[0]) <= max_length:\n",
    "            res.append(x)\n",
    "    return res\n",
    "\n",
    "\n",
    "def make_docs(data: List[Tuple[str, str]]) -> List[spacy.tokens.doc.Doc]:\n",
    "    \"\"\"\n",
    "    this will take a list of texts and labels\n",
    "    and transform them in spacy documents\n",
    "\n",
    "    data: list(tuple(text, label))\n",
    "\n",
    "    returns: List(spacy.Doc.doc)\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    nlp = en_core_web_lg.load()\n",
    "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total=len(data)):\n",
    "        # we need to set the (text)cat(egory) for each document\n",
    "        if label == 'True':\n",
    "            doc.cats[\"Definition\"] = 1\n",
    "            doc.cats[\"Not Definition\"] = 0\n",
    "        else:\n",
    "            doc.cats[\"Definition\"] = 0\n",
    "            doc.cats[\"Not Definition\"] = 1\n",
    "\n",
    "        docs.append(doc)\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "TRANSFORMER = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER)\n",
    "\n",
    "STORAGE_PATH = \"../../mars/definition_extraction/deft_corpus/data\"\n",
    "positive = \"DEFINITION\"\n",
    "negative = \"NOT DEFINITION\"\n",
    "\n",
    "print(\"Initializing...\")\n",
    "deft_loader = DeftCorpusLoader(STORAGE_PATH)\n",
    "trainframe, devframe, testframe = deft_loader.load_classification_data(preprocess=True, clean=True)\n",
    "\n",
    "wiki = create_from_wiki()\n",
    "wiki_sentences = [x[0] for x in wiki]\n",
    "wiki_labels = [x[1] for x in wiki]\n",
    "\n",
    "train_sentences, train_labels = list(trainframe[\"Sentence\"]), list(trainframe[\"HasDef\"])\n",
    "val_sentences, val_labels = list(devframe[\"Sentence\"]), list(devframe[\"HasDef\"])\n",
    "test_sentences, test_labels = list(testframe[\"Sentence\"]), list(testframe[\"HasDef\"])\n",
    "\n",
    "train_sentences = train_sentences + wiki_sentences[:int(len(wiki_sentences) * 7/10)]\n",
    "val_sentences = val_sentences + wiki_sentences[int(len(wiki_sentences) * 7/10):int(len(wiki_sentences) * 9/10)]\n",
    "test_sentences = test_sentences + wiki_sentences[int(len(wiki_sentences) * 9/10):]\n",
    "\n",
    "train_labels = train_labels + wiki_labels[:int(len(wiki_sentences) * 7/10)]\n",
    "val_labels = val_labels + wiki_labels[int(len(wiki_sentences) * 7/10):int(len(wiki_sentences) * 9/10)]\n",
    "test_labels = test_labels + wiki_labels[int(len(wiki_sentences) * 9/10):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing\n",
      "Creating model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_transform', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_158', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Tokenizing\")\n",
    "train_encodings = tokenizer(train_sentences, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_sentences, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_sentences, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_labels\n",
    "))\n",
    "\n",
    "print(\"Creating model\")\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(TRANSFORMER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=[\"accuracy\"])\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"../models/transformer-models/\" + TRANSFORMER, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "    save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(train_dataset.shuffle(1000).batch(batch_size),\n",
    "#           validation_data=val_dataset.shuffle(1000).batch(batch_size),\n",
    "#           callbacks = [es, mc],\n",
    "#           epochs=5,\n",
    "#           batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../../models/distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ../../models/distilbert-base-uncased and are newly initialized: ['dropout_198']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"../../models/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8632"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(test_dataset.batch(32))\n",
    "predictions = tf.math.softmax(preds.logits, axis=-1)\n",
    "y_preds = 1 * np.array(predictions[:,1] > 0.5) \n",
    "y_true = np.array(test_labels)\n",
    "\n",
    "accuracy_score(y_true, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_single_sentence(sentence, model, tokenizer):\n",
    "    \n",
    "    inputs = tokenizer(sentence, return_tensors=\"tf\")\n",
    "    inputs[\"labels\"] = tf.reshape(tf.constant(1), (-1, 1)) # Batch size 1\n",
    "    outputs = model(inputs)\n",
    "    predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99847597, 0.00152402]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_single_sentence(\"popular\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "predictions = tf.math.softmax(preds.logits, axis=-1)\n",
    "y_preds = 1 * np.array(predictions[:,1] > 0.5) \n",
    "y_true = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8226221079691517"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.782608695652174"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_true, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ({input_ids: (107,), attention_mask: (107,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}