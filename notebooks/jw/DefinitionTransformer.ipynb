{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import shutil\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "from typing import Tuple, List\n",
    "\n",
    "import en_core_web_lg\n",
    "import pandas as pd\n",
    "import spacy.tokens.doc\n",
    "import typer\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from mars.definition_extraction import DeftCorpusLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_to_spacy3(frame: pd.DataFrame, cats: list) -> List[Tuple]:\n",
    "    ret = []\n",
    "    sentences = frame['Sentence'].reset_index(drop=True)\n",
    "    for i in range(len(sentences)):\n",
    "        if cats[i]['DEFINITION'] is True:\n",
    "            cat = 1\n",
    "        else:\n",
    "            cat = 0\n",
    "\n",
    "        ret.append((sentences[i], cat))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def create_from_wiki(path='../../mars/definition_extraction/wcl_datasets_v1.2/wikipedia/', files=None) -> List[Tuple]:\n",
    "    if files is None:\n",
    "        files = {'wiki_bad.txt': 0, 'wiki_good.txt': 1}\n",
    "\n",
    "    file_sentences = {}\n",
    "\n",
    "    for f in files.keys():\n",
    "        filename = os.path.join(path, f)\n",
    "        defs = []\n",
    "        with open(filename, 'rb') as file:\n",
    "            lines = file.readlines()\n",
    "            lines = np.array([line.rstrip() for line in lines])\n",
    "\n",
    "        for line in lines[1::2]:\n",
    "            defs.append(str(line)[2:].split(':')[0])\n",
    "\n",
    "        lines = lines[::2]\n",
    "        for i, line in enumerate(lines):\n",
    "            lines[i] = str(line)[4:-2]\n",
    "        lines = lines.astype(str)\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            lines[i] = lines[i].replace('TARGET', defs[i])\n",
    "\n",
    "        lines = list(lines)\n",
    "        lines = [(str(line), int(files[f])) for line in lines]\n",
    "\n",
    "        file_sentences[f] = lines\n",
    "\n",
    "    file_sentences = list(file_sentences.values())[0] + list(file_sentences.values())[1]\n",
    "    return file_sentences\n",
    "\n",
    "\n",
    "def filter_out(sentences: List[Tuple], max_length: int) -> List[Tuple]:\n",
    "    res = []\n",
    "    for x in sentences:\n",
    "        if len(x[0]) <= max_length:\n",
    "            res.append(x)\n",
    "    return res\n",
    "\n",
    "\n",
    "def make_docs(data: List[Tuple[str, str]]) -> List[spacy.tokens.doc.Doc]:\n",
    "    \"\"\"\n",
    "    this will take a list of texts and labels\n",
    "    and transform them in spacy documents\n",
    "\n",
    "    data: list(tuple(text, label))\n",
    "\n",
    "    returns: List(spacy.Doc.doc)\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    nlp = en_core_web_lg.load()\n",
    "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total=len(data)):\n",
    "        # we need to set the (text)cat(egory) for each document\n",
    "        if label == 'True':\n",
    "            doc.cats[\"Definition\"] = 1\n",
    "            doc.cats[\"Not Definition\"] = 0\n",
    "        else:\n",
    "            doc.cats[\"Definition\"] = 0\n",
    "            doc.cats[\"Not Definition\"] = 1\n",
    "\n",
    "        docs.append(doc)\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "TRANSFORMER = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER)\n",
    "\n",
    "STORAGE_PATH = \"../../mars/definition_extraction/deft_corpus/data\"\n",
    "positive = \"DEFINITION\"\n",
    "negative = \"NOT DEFINITION\"\n",
    "\n",
    "print(\"Initializing...\")\n",
    "deft_loader = DeftCorpusLoader(STORAGE_PATH)\n",
    "trainframe, devframe, testframe = deft_loader.load_classification_data(preprocess=True, clean=True)\n",
    "\n",
    "wiki = create_from_wiki()\n",
    "wiki_sentences = [x[0] for x in wiki]\n",
    "wiki_labels = [x[1] for x in wiki]\n",
    "\n",
    "train_sentences, train_labels = list(trainframe[\"Sentence\"]), list(trainframe[\"HasDef\"])\n",
    "val_sentences, val_labels = list(devframe[\"Sentence\"]), list(devframe[\"HasDef\"])\n",
    "test_sentences, test_labels = list(testframe[\"Sentence\"]), list(testframe[\"HasDef\"])\n",
    "\n",
    "train_sentences = train_sentences + wiki_sentences[:int(len(wiki_sentences) * 7/10)]\n",
    "val_sentences = val_sentences + wiki_sentences[int(len(wiki_sentences) * 7/10):int(len(wiki_sentences) * 9/10)]\n",
    "test_sentences = test_sentences + wiki_sentences[int(len(wiki_sentences) * 9/10):]\n",
    "\n",
    "train_labels = train_labels + wiki_labels[:int(len(wiki_sentences) * 7/10)]\n",
    "val_labels = val_labels + wiki_labels[int(len(wiki_sentences) * 7/10):int(len(wiki_sentences) * 9/10)]\n",
    "test_labels = test_labels + wiki_labels[int(len(wiki_sentences) * 9/10):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing\n",
      "Creating model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_transform', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_158', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Tokenizing\")\n",
    "train_encodings = tokenizer(train_sentences, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_sentences, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_sentences, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_labels\n",
    "))\n",
    "\n",
    "print(\"Creating model\")\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(TRANSFORMER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=[\"accuracy\"])\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"../models/transformer-models/\" + TRANSFORMER, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "    save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(train_dataset.shuffle(1000).batch(batch_size),\n",
    "#           validation_data=val_dataset.shuffle(1000).batch(batch_size),\n",
    "#           callbacks = [es, mc],\n",
    "#           epochs=5,\n",
    "#           batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../../models/distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ../../models/distilbert-base-uncased and are newly initialized: ['dropout_198']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"../../models/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8632"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(test_dataset.batch(32))\n",
    "predictions = tf.math.softmax(preds.logits, axis=-1)\n",
    "y_preds = 1 * np.array(predictions[:,1] > 0.5) \n",
    "y_true = np.array(test_labels)\n",
    "\n",
    "accuracy_score(y_true, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_single_sentence(sentence, model, tokenizer):\n",
    "    \n",
    "    inputs = tokenizer(sentence, return_tensors=\"tf\")\n",
    "    inputs[\"labels\"] = tf.reshape(tf.constant(1), (-1, 1)) # Batch size 1\n",
    "    outputs = model(inputs)\n",
    "    predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99847597, 0.00152402]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_single_sentence(\"popular\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "predictions = tf.math.softmax(preds.logits, axis=-1)\n",
    "y_preds = 1 * np.array(predictions[:,1] > 0.5) \n",
    "y_true = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8226221079691517"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.782608695652174"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_true, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ({input_ids: (107,), attention_mask: (107,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mars.definition_model import DistilBertBaseUncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 12:05:19.833309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 12:05:19.842407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 12:05:19.842732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 12:05:19.843895: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-16 12:05:19.846180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 12:05:19.846651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 12:05:19.847091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 12:05:20.724509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 12:05:20.724967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 12:05:20.724985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2021-11-16 12:05:20.725403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 12:05:20.725450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3947 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2021-11-16 12:05:20.919565: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at ../../models/distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "dbc = DistilBertBaseUncased(\"../../models/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dbc.predict_single_sentence(\"The term is containing something good\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dbc.predict_single_sentence(\"Something\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import mars.db\n",
    "import spacy\n",
    "from mars.db import collections\n",
    "from mars import logging\n",
    "from mars.db.db_fields import (\n",
    "    CONTENT,\n",
    "    DOC_ID,\n",
    "    SEGMENT_ID,\n",
    "    FILENAME,\n",
    "    ID,\n",
    "    SOURCE,\n",
    "    URL,\n",
    "    id_to_key,\n",
    "    SENTENCE,\n",
    "    HTML_TAG,\n",
    "    IS_HEADER,\n",
    "    SEQUENCE_NUMBER,\n",
    "    SENTENCE_NUMBER,\n",
    "    IS_DEFINITION,\n",
    "    SENTENCE_DOC_ID\n",
    ")\n",
    "from mars.definition_model import DistilBertBaseUncased\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "logger = logging.new_logger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def get_docs_range(key_min: int, key_max: int):\n",
    "    res = []\n",
    "    for i in range(key_min, key_max + 1):\n",
    "        res.append(\"Documents/\" + str(i))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_min = 1158\n",
    "key_max = 1170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_docs_query = f\"FOR u IN {collections.DOCUMENTS} \" \\\n",
    "                     f\"FILTER TO_NUMBER(u._key) >= {key_min} && TO_NUMBER(u._key) <= {key_max}\" \\\n",
    "                     f\" RETURN DISTINCT u.{ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = mars.db.database.AQLQuery(get_docs_query, 10, rawResults=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': ['Documents/1158', 'Documents/1170'],\n",
       " 'hasMore': False,\n",
       " 'cached': False,\n",
       " 'extra': {'warnings': [],\n",
       "  'stats': {'writesExecuted': 0,\n",
       "   'writesIgnored': 0,\n",
       "   'scannedFull': 0,\n",
       "   'scannedIndex': 6221,\n",
       "   'filtered': 6219,\n",
       "   'httpRequests': 0,\n",
       "   'executionTime': 0.021561860980000347,\n",
       "   'peakMemoryUsage': 32768}},\n",
       " 'error': False,\n",
       " 'code': 201}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_done_docs_query = f\"FOR u IN {collections.SENTENCES} \" \\\n",
    "                      f\"FILTER u.{SENTENCE_DOC_ID} IN TO_ARRAY({get_docs_range(key_min, key_max)}) \" \\\n",
    "                      f\"FILTER IS_NUMBER(u.{IS_DEFINITION}) == true \" \\\n",
    "                      f\"FILTER u.{IS_DEFINITION} >= 0 \" \\\n",
    "                      f\"RETURN DISTINCT u.{SENTENCE_DOC_ID} \"\n",
    "                      \n",
    "                      \n",
    "done_docs = mars.db.database.AQLQuery(get_done_docs_query, 10, rawResults=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': [],\n",
       " 'hasMore': False,\n",
       " 'cached': False,\n",
       " 'extra': {'warnings': [],\n",
       "  'stats': {'writesExecuted': 0,\n",
       "   'writesIgnored': 0,\n",
       "   'scannedFull': 3994205,\n",
       "   'scannedIndex': 0,\n",
       "   'filtered': 3994205,\n",
       "   'httpRequests': 0,\n",
       "   'executionTime': 6.239703973988071,\n",
       "   'peakMemoryUsage': 32768}},\n",
       " 'error': False,\n",
       " 'code': 201}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done_docs.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_docs = set(list(done_docs))\n",
    "all_docs = set(list(all_docs))\n",
    "todo_docs = list(all_docs - done_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Documents/1170', 'Documents/1158']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "todo_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = todo_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArangoDoc '_id: Sentences/12039915, _key: 12039915, _rev: _dQf4Wua---': <store: {'sentence_number': 1, 'html_tag': 'p', 'is_header': False, 'sequence_number': 248, 'source_segment_id': 'SegmentedTexts/480300', 'source_doc_id': 'Documents/1170', 'sentence': 'No\\xa045/2001 and Decision No\\xa01247/2002/EC (OJ L\\xa0295, 21.11.2018, p.\\xa039).', 'is_definition': 0.0009291817550547421}>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, sentence in enumerate(todo_sentences):\n",
    "    text = sentence[SENTENCE]\n",
    "    counter = 0\n",
    "    sentence[IS_DEFINITION] = float(dbc.predict_single_sentence(text))\n",
    "    sentence.save()\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REGULATION (EU) 2021/1232 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "todo_sentences[0][SENTENCE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "todo_sentences = [\n",
    "    sentence for sentence in all_sentences if sentence[ID] not in done_sentences\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'segment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1793/908725436.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mall_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m todo_sentences = [\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_sentences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdone_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m ]\n",
      "\u001b[0;32m/tmp/ipykernel_1793/908725436.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mall_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m todo_sentences = [\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_sentences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdone_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'segment' is not defined"
     ]
    }
   ],
   "source": [
    "get_all_sentences_query = f\"FOR u IN {collections.SENTENCES} FILTER u.{SENTENCE_DOC_ID} == \\\"{doc}\\\" RETURN u\"\n",
    "all_sentences = mars.db.database.AQLQuery(\n",
    "    get_all_sentences_query, 10000\n",
    ")\n",
    "\n",
    "get_done_sentences_query = f\"FOR u IN {collections.SENTENCES} \" \\\n",
    "                           f\"FILTER u.{SENTENCE_DOC_ID} == \\\"{doc}\\\" \" \\\n",
    "                           f\"FILTER IS_NUMBER(u.{IS_DEFINITION}) == true \" \\\n",
    "                           f\"FILTER u.{IS_DEFINITION} >= 0 \" \\\n",
    "                           f\"RETURN DISTINCT u.{ID} \"\n",
    "\n",
    "done_sentences = mars.db.database.AQLQuery(\n",
    "    get_done_sentences_query, 10000, rawResults=True\n",
    ")\n",
    "done_sentences = set(list(done_sentences))\n",
    "all_sentences = list(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009291817550547421"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbc.predict_single_sentence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArangoDoc '_id: Sentences/12039247, _key: 12039247, _rev: _dJ0wTtq---': <store: {'sentence_number': 0, 'html_tag': 'p', 'is_header': False, 'sequence_number': 0, 'source_segment_id': 'SegmentedTexts/479804', 'source_doc_id': 'Documents/1170', 'sentence': 'REGULATION (EU) 2021/1232 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL'}>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "todo_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mars.sentence_definition_scoring import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../../models/distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ../../models/distilbert-base-uncased and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11/16/2021 12:41:08 (0.0%) Processing document Documents/15310248. Sentences count: 2\n"
     ]
    }
   ],
   "source": [
    "sentence_definition_scoring(15310248, 15310248, \"../../models/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
