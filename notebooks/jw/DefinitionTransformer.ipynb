{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import tqdm\n",
    "import shutil\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMER = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "import en_core_web_lg\n",
    "import pandas as pd\n",
    "import spacy.tokens.doc\n",
    "import typer\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from mars.definition_extraction import DeftCorpusLoader\n",
    "\n",
    "STORAGE_PATH = \"../../mars/definition_extraction/deft_corpus/data\"\n",
    "positive = \"DEFINITION\"\n",
    "negative = \"NOT DEFINITION\"\n",
    "\n",
    "\n",
    "def transform_to_spacy3(frame: pd.DataFrame, cats: list) -> List[Tuple]:\n",
    "    ret = []\n",
    "    sentences = frame['Sentence'].reset_index(drop=True)\n",
    "    for i in range(len(sentences)):\n",
    "        if cats[i]['DEFINITION'] is True:\n",
    "            cat = 1\n",
    "        else: \n",
    "            cat = 0\n",
    "            \n",
    "        ret.append((sentences[i], cat))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def create_from_wiki(path='../../mars/definition_extraction/wcl_datasets_v1.2/wikipedia/', files=None) -> List[Tuple]:\n",
    "    if files is None:\n",
    "        files = {'wiki_bad.txt': 0, 'wiki_good.txt': 1}\n",
    "\n",
    "    file_sentences = {}\n",
    "\n",
    "    for f in files.keys():\n",
    "        filename = os.path.join(path, f)\n",
    "        defs = []\n",
    "        with open(filename, 'rb') as file:\n",
    "            lines = file.readlines()\n",
    "            lines = np.array([line.rstrip() for line in lines])\n",
    "\n",
    "        for line in lines[1::2]:\n",
    "            defs.append(str(line)[2:].split(':')[0])\n",
    "\n",
    "        lines = lines[::2]\n",
    "        for i, line in enumerate(lines):\n",
    "            lines[i] = str(line)[4:-2]\n",
    "        lines = lines.astype(str)\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            lines[i] = lines[i].replace('TARGET', defs[i])\n",
    "\n",
    "        lines = list(lines)\n",
    "        lines = [(str(line), int(files[f])) for line in lines]\n",
    "\n",
    "        file_sentences[f] = lines\n",
    "\n",
    "    file_sentences = list(file_sentences.values())[0] + list(file_sentences.values())[1]\n",
    "    return file_sentences\n",
    "\n",
    "def filter_out(sentences:List[Tuple], max_length:int)->List[Tuple]:\n",
    "\n",
    "    res = []\n",
    "    for x in sentences:\n",
    "        if len(x[0]) <= max_length:\n",
    "            res.append(x)\n",
    "    return res\n",
    "\n",
    "def make_docs(data: List[Tuple[str, str]]) -> List[spacy.tokens.doc.Doc]:\n",
    "    \"\"\"\n",
    "    this will take a list of texts and labels\n",
    "    and transform them in spacy documents\n",
    "\n",
    "    data: list(tuple(text, label))\n",
    "\n",
    "    returns: List(spacy.Doc.doc)\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    nlp = en_core_web_lg.load()\n",
    "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total=len(data)):\n",
    "        # we need to set the (text)cat(egory) for each document\n",
    "        if label == 'True':\n",
    "            doc.cats[\"Definition\"] = 1\n",
    "            doc.cats[\"Not Definition\"] = 0\n",
    "        else:\n",
    "            doc.cats[\"Definition\"] = 0\n",
    "            doc.cats[\"Not Definition\"] = 1\n",
    "\n",
    "        docs.append(doc)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "Transforming to spacy3 format\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing...\")\n",
    "deft_loader = DeftCorpusLoader(STORAGE_PATH)\n",
    "trainframe, devframe, testframe = deft_loader.load_classification_data(preprocess=True, clean=True)\n",
    "train_cats = [{positive: bool(y), negative: not bool(y)} for y in trainframe[\"HasDef\"]]\n",
    "dev_cats = [{positive: bool(y), negative: not bool(y)} for y in devframe[\"HasDef\"]]\n",
    "test_cats = [{positive: bool(y), negative: not bool(y)} for y in testframe[\"HasDef\"]]\n",
    "\n",
    "print(\"Transforming to spacy3 format\")\n",
    "wiki = create_from_wiki()\n",
    "train_df = transform_to_spacy3(trainframe, train_cats)\n",
    "dev_df = transform_to_spacy3(devframe, dev_cats)\n",
    "test_df = transform_to_spacy3(testframe, test_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = wiki[:int((3/4 * len(wiki)))] + train_df\n",
    "dev_df = wiki[int((3/4 * len(wiki))):] + dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_labels = list(trainframe[\"Sentence\"]), list(trainframe[\"HasDef\"])\n",
    "val_sentences, val_labels = list(devframe[\"Sentence\"]), list(devframe[\"HasDef\"])\n",
    "test_sentences, test_labels = list(testframe[\"Sentence\"]), list(testframe[\"HasDef\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_sentences, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_sentences, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_sentences, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-03 12:54:24.519128: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 154414080 exceeds 10% of free system memory.\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFRobertaForSequenceClassification\n",
    "model = TFRobertaForSequenceClassification.from_pretrained(TRANSFORMER)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_197', 'pre_classifier', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "975/975 [==============================] - 367s 369ms/step - loss: 0.6217 - val_loss: 0.7991\n",
      "Epoch 2/5\n",
      "975/975 [==============================] - 367s 377ms/step - loss: 0.6241 - val_loss: 0.7656\n",
      "Epoch 3/5\n",
      "975/975 [==============================] - 2612s 3s/step - loss: 0.6231 - val_loss: 0.7862\n",
      "Epoch 4/5\n",
      "  4/975 [..............................] - ETA: 1:26:54 - loss: 1.0504"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss)\n",
    "\n",
    "model.fit(train_dataset.shuffle(1000).batch(16),\n",
    "          validation_data = val_dataset.shuffle(1000).batch(16), \n",
    "          epochs=5,\n",
    "          batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
